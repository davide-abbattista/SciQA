{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "collapsed_sections": [
        "Oq7V4TjglTbD",
        "HlR6aZbGlZqc",
        "jJ2Zm0jYlmvq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scientific Question-Answering with Citations - Davide Abbattista\n"
      ],
      "metadata": {
        "id": "AqVAfJcRkz18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A client requests an AI-powered solution that can accurately respond to user queries related to scientific topics. They have gathered a [substantial collection of documents](https://huggingface.co/datasets/loukritia/science-journal-for-kids-data) and wish for the system to leverage this data to provide accurate and reliable answers to user queries.\n",
        "\n",
        "Here, the proposed solution is described and implemented."
      ],
      "metadata": {
        "id": "xckTRTiBBwOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Description"
      ],
      "metadata": {
        "id": "-pmbGmmTr53b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Scientific Question-Answering System is an end-to-end solution designed to provide accurate, contextually relevant, and citation-supported answers to user queries. The system leverages state-of-the-art NLP techniques for document retrieval, answer generation, and consistency verification. It is built to process a dataset of scientific abstracts and provide users with reliable answers while ensuring transparency through citations and warnings when inconsistencies are detected.\n",
        "\n",
        "**Overview of Query and Document Processing**\n",
        "\n",
        "When a user submits a query, the system follows a structured process to retrieve, process, and generate a response. Abstracts from the dataset are first preprocessed and embedded into a vector space for efficient similarity matching. The system then retrieves the most relevant documents, and uses these documents as context for generating a coherent answer. The entire workflow ensures that the generated answer is accurate, consistent, and traceable to its source documents.\n",
        "\n",
        "**Dataset Preparation and Preprocessing**\n",
        "\n",
        "The system starts by loading and inspecting the dataset using the Dataset Handler:\n",
        "-\tData Loading: The dataset, provided in CSV format, is loaded into a structured DataFrame. Key columns include the original abstracts from academic papers and their simplified “Kids Abstracts.”\n",
        "-\tInspection: The system checks for missing values to ensure completeness and analyzes the token lengths of abstracts to verify compatibility with the model's token limits.\n",
        "-\tSummary: A high-level overview of the dataset, including data types and sample entries, is displayed.\n",
        "\n",
        "**Embedding Creation and Similarity Search**\n",
        "\n",
        "To efficiently retrieve relevant documents, the Embedding Indexer encodes the dataset into dense vector embeddings:\n",
        "-\tEmbedding Model: The [multi-qa-mpnet-base-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1) model is used to generate embeddings. This model is specifically designed for semantic search. It has been trained on question-answer pairs from diverse sources and ensures high-quality semantic representation of textual data.\n",
        "-\tIndexing: The embeddings are stored in a [FAISS](https://faiss.ai/) index, a fast and scalable library for nearest neighbor search. The system uses cosine similarity as the metric to retrieve the most relevant documents for a given query.\n",
        "-\tQuery Matching: When a query is submitted, it is also encoded into a dense vector. This query embedding is then matched against the FAISS index to retrieve the top-k documents that are semantically closest to the query.\n",
        "\n",
        "The use of dense embeddings and FAISS ensures scalability, making the system capable of handling large datasets efficiently.\n",
        "\n",
        "**Query Answering and Consistency Verification**\n",
        "\n",
        "Once relevant documents are retrieved, the Query Answering module processes the query and documents to generate a contextually appropriate answer:\n",
        "\n",
        "1. Answer Generation:\n",
        " -\tA zero-shot prompting approach is used with the [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) Seq2Seq model to generate a detailed answer. The context for this model includes the abstracts of the retrieved documents and the query itself.\n",
        " - The model excels at generating coherent, human-readable answers, leveraging its pre-trained capabilities on a variety of NLP tasks.\n",
        "\n",
        "2. Consistency Verification:\n",
        " -\tTo ensure the generated answer is aligned with the source documents, the system calculates semantic similarity between the generated answer and potential answer spans extracted from the retrieved documents.\n",
        " -\tThe [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) model identifies potential answer spans, and the [multi-qa-mpnet-base-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1) model computes their embeddings.\n",
        " -\tThe similarity between the generated answer's embedding and each span's embedding is evaluated. If at least one similarity score meets or exceeds a predefined threshold, the answer is considered consistent.\n",
        " - The retrieved documents associated to the similarity scores above the threshold are used as references in the system's answer.\n",
        "\n",
        "3. Citations and Warnings:\n",
        " -\tCitations for the documents that meet the consistency check, used in generating the answer, are appended to provide transparency and traceability.\n",
        " -\tIf the consistency check fails, the system flags the answer with a warning, alerting the user to potential inaccuracies.\n",
        "\n",
        "**System Integration and Workflow**\n",
        "\n",
        "The Scientific QA System orchestrates the entire process, combining the dataset preparation, embedding indexing, and query answering components:\n",
        "\n",
        "1. Initialization:\n",
        " - The dataset is loaded and preprocessed.\n",
        " - \tDense vector embeddings are created for all abstracts, and a FAISS index is built for similarity search.\n",
        " -\tThe query-answering model and consistency-checking mechanisms are initialized.\n",
        "\n",
        "2.\tQuery Handling:\n",
        " -\tThe user's query is encoded into an embedding and matched against the FAISS index to retrieve the most relevant documents.\n",
        " -\tThe retrieved documents are used to generate an answer, which is then validated for consistency.\n",
        "\n",
        "3. Answer Delivery:\n",
        " -\tThe system returns the generated answer, along with citations for the source documents that meet the consistency check.\n",
        " -\tIf the consistency check fails, the system includes a warning.\n",
        "\n",
        "**Justification of Implementation Choices**\n",
        "-\tDense Embeddings and FAISS: Using dense embeddings ensures robust semantic matching, while FAISS provides a scalable solution for similarity search, enabling the system to handle large datasets efficiently.\n",
        "-\tZero-Shot Prompting: The [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) model is leveraged for its strong generalization capabilities, ensuring high-quality answers without task-specific fine-tuning.\n",
        "-\tConsistency Checks: The consistency verification mechanism enhances the reliability of the system, ensuring that the answers align with the source documents.\n",
        "-\tTransparency: Citations and warnings provide users with the necessary context to evaluate the trustworthiness of the generated answers."
      ],
      "metadata": {
        "id": "hsmZtb8_SNH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation"
      ],
      "metadata": {
        "id": "dfGHXbT3mK9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The required environment is set up by installing the necessary Python libraries and importing them into the notebook.\n",
        "\n",
        "Here is what each library is used for:\n",
        "- [pandas](https://pandas.pydata.org/docs/): A fast, powerful, flexible and easy to use data analysis and manipulation tool.\n",
        "- [sentence-transformers](https://sbert.net/): A module for using state-of-the-art text embedding models\n",
        "- [faiss-cpu](https://github.com/facebookresearch/faiss/wiki/): A library for efficient similarity search of dense vectors.\n",
        "- [transformers](https://huggingface.co/docs/transformers/index): An Hugging Face's library that provides APIs and tools to easily download state-of-the-art pretrained models.\n",
        "- [torch](https://pytorch.org/docs/stable/index.html): PyTorch, an optimized tensor library for deep learning using GPUs and CPUs."
      ],
      "metadata": {
        "id": "EZk7RH6vBtkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3EJIlnZ9liM",
        "outputId": "91fbd516-52a7-40fc-a0eb-ff1d541e0390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ],
      "source": [
        "# Installing required libraries\n",
        "# !pip install pandas sentence-transformers transformers torch (requirements already satisfied)\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for data manipulation, embedding creation, indexing, and question-answering tasks\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "from sentence_transformers import SentenceTransformer  # For creating sentence embeddings\n",
        "import faiss  # For fast similarity search using dense vector embeddings\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM  # For question-answering and Seq2Seq models\n",
        "import torch  # For GPU/CPU computation"
      ],
      "metadata": {
        "id": "DE7ZfmLwnZ4R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Implementation"
      ],
      "metadata": {
        "id": "i1KvWPQimS6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Handler\n"
      ],
      "metadata": {
        "id": "Oq7V4TjglTbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Load and inspect the dataset.\n",
        "\n",
        "**Functionality**\n",
        "-\tReads the dataset provided in CSV format.\n",
        "-\tChecks for missing values and token lengths.\n",
        "-\tSummarizes dataset properties, such as data types and sample entries, to facilitate understanding."
      ],
      "metadata": {
        "id": "07UtzaH7sKYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class to manage dataset loading, inspection, and preprocessing\n",
        "class DatasetHandler:\n",
        "    def __init__(self, dataset_url):\n",
        "        \"\"\"\n",
        "        Initialize the DatasetHandler.\n",
        "        - dataset_url: URL to the dataset (CSV format).\n",
        "        \"\"\"\n",
        "        self.dataset_url = dataset_url  # URL to the dataset\n",
        "        self.data = None  # Placeholder for the dataset\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")  # Tokenizer for analyzing token lengths\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"\n",
        "        Load the dataset from the specified URL or path.\n",
        "        Prints the dataset's columns to verify successful loading.\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(self.dataset_url)  # Load dataset using pandas\n",
        "        print(\"Dataset loaded successfully.\")\n",
        "        print(\"\\nColumns:\")\n",
        "        print(*self.data.columns, sep=\", \")  # Display available columns\n",
        "\n",
        "    def check_missing_values(self):\n",
        "        \"\"\"\n",
        "        Check for missing values in the dataset.\n",
        "        Reports the count of missing values for each column.\n",
        "        \"\"\"\n",
        "        missing_values = self.data.isnull().sum()  # Count missing values per column\n",
        "        print(\"\\nMissing Values Summary:\")\n",
        "        if missing_values.sum() == 0:  # If no missing values\n",
        "            print(\"No missing values detected.\")\n",
        "        else:\n",
        "            print(missing_values[missing_values > 0])  # Report only columns with missing values\n",
        "\n",
        "    def check_abstract_token_lengths(self):\n",
        "        \"\"\"\n",
        "        Analyze token lengths for the 'Abstract (Original academic paper)' and 'Kids Abstract' columns.\n",
        "        Helps ensure the content is within token limits for the chosen model.\n",
        "        \"\"\"\n",
        "        # Calculate token counts for the original abstract\n",
        "        self.data['Original Abstract Length (tokens)'] = self.data['Abstract (Original academic paper)'].apply(\n",
        "            lambda x: len(self.tokenizer.tokenize(str(x), truncation=True, max_length=2048)) if pd.notnull(x) else 0)\n",
        "\n",
        "        # Calculate token counts for the Kids abstract\n",
        "        self.data['Kids Abstract Length (tokens)'] = self.data['Kids Abstract'].apply(\n",
        "            lambda x: len(self.tokenizer.tokenize(str(x), truncation=True, max_length=2048)) if pd.notnull(x) else 0)\n",
        "\n",
        "        # Calculate combined token counts for both abstracts\n",
        "        self.data['Combined Abstract Length (tokens)'] = (\n",
        "            self.data['Original Abstract Length (tokens)'] + self.data['Kids Abstract Length (tokens)']\n",
        "        )\n",
        "\n",
        "        # Display summary statistics for token lengths\n",
        "        print(\"\\nToken Length Statistics:\")\n",
        "        print(f\"Original Abstract - Min: {self.data['Original Abstract Length (tokens)'].min()}, \"\n",
        "              f\"Max: {self.data['Original Abstract Length (tokens)'].max()}, \"\n",
        "              f\"Mean: {self.data['Original Abstract Length (tokens)'].mean():.2f}\")\n",
        "        print(f\"Kids Abstract - Min: {self.data['Kids Abstract Length (tokens)'].min()}, \"\n",
        "              f\"Max: {self.data['Kids Abstract Length (tokens)'].max()}, \"\n",
        "              f\"Mean: {self.data['Kids Abstract Length (tokens)'].mean():.2f}\")\n",
        "        print(f\"Combined Abstract - Min: {self.data['Combined Abstract Length (tokens)'].min()}, \"\n",
        "              f\"Max: {self.data['Combined Abstract Length (tokens)'].max()}, \"\n",
        "              f\"Mean: {self.data['Combined Abstract Length (tokens)'].mean():.2f}\")\n",
        "\n",
        "    def summarize_dataset(self):\n",
        "        \"\"\"\n",
        "        Provide a high-level summary of the dataset.\n",
        "        Prints data types, memory usage, and previews the first few rows.\n",
        "        \"\"\"\n",
        "        print(\"\\nDataset Summary:\")\n",
        "        print(self.data.info())  # Summary of data types and memory usage\n",
        "        print(\"\\nFirst few rows of the dataset:\")\n",
        "        print(self.data.head())  # Preview dataset rows"
      ],
      "metadata": {
        "id": "dc8AOVv5555h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Indexer\n"
      ],
      "metadata": {
        "id": "HlR6aZbGlZqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Create a searchable index of document embeddings for fast and efficient similarity retrieval.\n",
        "\n",
        "**Functionality**\n",
        "- Uses the [multi-qa-mpnet-base-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1) model, from [SentenceTransformer](https://huggingface.co/sentence-transformers) library, to encode textual data into dense vector embeddings.\n",
        "- Builds a [FAISS](https://faiss.ai/) index to allow for efficient similarity search based on cosine similarity.\n",
        "- Supports searching for the most relevant documents using query embeddings."
      ],
      "metadata": {
        "id": "ZROdS6TmKNQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for embedding generation and FAISS indexing\n",
        "class EmbeddingIndexer:\n",
        "    def __init__(self, semantic_search_model='multi-qa-mpnet-base-cos-v1'):\n",
        "        \"\"\"\n",
        "        Initialize the embedding model and FAISS index.\n",
        "        - semantic_search_model: Name of the sentence-transformers model used to generate embeddings.\n",
        "        \"\"\"\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "        self.model = SentenceTransformer(semantic_search_model, device=device)  # Load embedding model\n",
        "        self.index = None  # Placeholder for FAISS index\n",
        "\n",
        "    def create_embeddings(self, texts):\n",
        "        \"\"\"\n",
        "        Generate embeddings for a list of texts.\n",
        "        - texts: List of textual data to encode.\n",
        "        Returns: Numpy array of embeddings.\n",
        "        \"\"\"\n",
        "        embeddings = self.model.encode(texts, convert_to_tensor=False)  # Encode texts into embeddings\n",
        "        return embeddings\n",
        "\n",
        "    def build_index(self, embeddings):\n",
        "        \"\"\"\n",
        "        Build a FAISS index for fast similarity searches.\n",
        "        - embeddings: Array of dense embeddings to index.\n",
        "        \"\"\"\n",
        "        dimension = embeddings.shape[1]  # Dimensionality of embeddings\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Create an index for cosine similarity search\n",
        "        self.index.add(embeddings)  # Add embeddings to the index\n",
        "        print(f\"\\nFAISS index built with {self.index.ntotal} entries.\")  # Confirm number of entries indexed\n",
        "\n",
        "    def search(self, query_embedding, top_k=3):\n",
        "        \"\"\"\n",
        "        Search the FAISS index for the top-k most relevant embeddings.\n",
        "        - query_embedding: Query embedding to match against the index.\n",
        "        - top_k: Number of top results to return.\n",
        "        Returns: Distances and indices of top-k results.\n",
        "        \"\"\"\n",
        "        distances, indices = self.index.search(query_embedding, top_k)  # Perform search\n",
        "        return distances, indices"
      ],
      "metadata": {
        "id": "AWA2rh4TldU-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Answering"
      ],
      "metadata": {
        "id": "jJ2Zm0jYlmvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Generate accurate, context-aware answers to user queries and verify their consistency with the dataset.\n",
        "\n",
        "**Functionality**\n",
        "-\tRetrieves documents using [FAISS](https://faiss.ai/).\n",
        "-\tGenerates answers using the [google/flan-t5-large](https://huggingface.co/google/flan-t5-large) Seq2Seq model using a zero-shot prompting and the retrieved document as context.\n",
        "-\tVerifies the consistency of the generated answers by calculating semantic similarity with potential answer spans extracted from the retrieved documents:\n",
        "  - [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) is used to obtain potential answer spans;\n",
        "  - [multi-qa-mpnet-base-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1) model is used to compute the embedding of the Seq2Seq generated answer and the ones of the potential answer spans;\n",
        "  - the similarity between the answer's embedding and each span's embedding is computed;\n",
        "  - the generated answer is cosidered consistent if at least one similarity score meets or exceeds the threshold.\n",
        "-\tAppends citations for documents whose related similarity score is above the threshold and warns users if consistency is low."
      ],
      "metadata": {
        "id": "xZAmEcxxsVS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for handling query answering and consistency verification\n",
        "class QueryAnswering:\n",
        "    def __init__(self, nlp_model='google/flan-t5-large', qa_model='deepset/roberta-base-squad2', semantic_search_model='multi-qa-mpnet-base-cos-v1', consistency_threshold=0.5, data=None):\n",
        "        \"\"\"\n",
        "        Initialize the query answering system.\n",
        "        - nlp_model: Name of the Seq2Seq natural language model for answer generation.\n",
        "        - qa_model: Name of the Extractive QA model for extracting potential answers.\n",
        "        - semantic_search_model: Name of the sentence-transformers model used to generate embeddings.\n",
        "        - consistency_threshold: Similarity score threshold to determine consistency (default: 0.5).\n",
        "        - data: DataFrame containing the dataset.\n",
        "        \"\"\"\n",
        "        device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(nlp_model)  # Load tokenizer for Seq2Seq model\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(nlp_model).to(device)  # Load Seq2Seq model\n",
        "        self.data = data  # Dataset for reference\n",
        "        self.qa_pipeline = pipeline(\"question-answering\", model=qa_model, device=device)  # QA pipeline using the Extractive QA model\n",
        "        self.consistency_threshold = consistency_threshold # Consistency threshold\n",
        "        self.similarity_model = SentenceTransformer(semantic_search_model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Similarity model for consistency checks\n",
        "\n",
        "    def verify_answer_consistency(self, query, answer, retrieved_indices):\n",
        "        \"\"\"\n",
        "        Verify if the generated answer is consistent with the retrieved documents and identify the relevant ones.\n",
        "        - query: User query string.\n",
        "        - answer: Generated answer string.\n",
        "        - retrieved_indices: Indices of documents retrieved by FAISS.\n",
        "        Returns:\n",
        "            - is_consistent (bool): True if the answer is consistent with at least one retrieved document, False otherwise.\n",
        "            - relevant_indices (list): Indices of documents that are consistent with the generated answer.\n",
        "        \"\"\"\n",
        "        potential_spans = []  # Store possible answer spans extracted from the retrieved documents\n",
        "\n",
        "        # Loop through the indices of the retrieved documents\n",
        "        for idx in retrieved_indices:\n",
        "            # Construct a combined context from the original and simplified abstracts\n",
        "            context = (\n",
        "                f\"Original Abstract: {self.data.iloc[idx]['Abstract (Original academic paper)']}\\n\"\n",
        "                f\"Kids Abstract: {self.data.iloc[idx]['Kids Abstract']}\"\n",
        "            )\n",
        "            # Use the QA model to extract a potential answer from the context\n",
        "            result = self.qa_pipeline(question=query, context=context)\n",
        "            potential_spans.append(result[\"answer\"])  # Append the extracted answer to the spans list\n",
        "\n",
        "        # Generate embeddings for the generated answer\n",
        "        answer_embedding = self.similarity_model.encode(answer, convert_to_tensor=False)\n",
        "        # Generate embeddings for the extracted potential spans\n",
        "        span_embeddings = self.similarity_model.encode(potential_spans, convert_to_tensor=False)\n",
        "\n",
        "        # Compute similarity scores between the answer and each potential span\n",
        "        similarities = [\n",
        "            self.similarity_model.similarity(answer_embedding, span_embedding)\n",
        "            for span_embedding in span_embeddings\n",
        "        ]\n",
        "\n",
        "        # Identify relevant documents where similarity meets or exceeds the threshold\n",
        "        relevant_indices = [\n",
        "            retrieved_indices[i] for i, score in enumerate(similarities) if score >= self.consistency_threshold\n",
        "        ]\n",
        "\n",
        "        # Determine consistency: True if there is at least one relevant document\n",
        "        is_consistent = len(relevant_indices) > 0\n",
        "\n",
        "        return is_consistent, relevant_indices\n",
        "\n",
        "    def generate_answer(self, query, retrieved_indices):\n",
        "        \"\"\"\n",
        "        Generate an answer to the query using the retrieved relevant documents.\n",
        "        - query: User query string.\n",
        "        - retrieved_indices: Indices of documents retrieved by FAISS.\n",
        "        Returns: Generated answer with citations and a warning if consistency is low.\n",
        "        \"\"\"\n",
        "        # Extract relevant documents from the dataset\n",
        "        retrieved_docs = self.data.iloc[retrieved_indices]\n",
        "\n",
        "        # Combine abstracts from retrieved documents into a single context string\n",
        "        context = \"\\n\".join(\n",
        "            f\"Title: {row['Title']}\\n\"\n",
        "            f\"Original Abstract: {row['Abstract (Original academic paper)']}\\n\"\n",
        "            f\"Kids Abstract: {row['Kids Abstract']}\\n\"\n",
        "            for _, row in retrieved_docs.iterrows()\n",
        "        )\n",
        "\n",
        "        # Prepare input text for the Seq2Seq model\n",
        "        input_text = (\n",
        "            f\"You are a scientific assistant specialized in providing accurate answers based on provided research articles.\\n\"\n",
        "            f\"Your task is to use the given context to answer the question precisely.\\n\"\n",
        "            f\"Focus on clarity and avoid adding unsupported information.\\n\\n\"\n",
        "            f\"Context:\\n{context}\\n\\n\"\n",
        "            f\"Question: {query}\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "\n",
        "        # Tokenize the input text and prepare it for the model\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048, padding='max_length').to(self.model.device)\n",
        "        outputs = self.model.generate(**inputs, max_length=200)  # Generate the answer\n",
        "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)  # Decode the output\n",
        "\n",
        "        # Verify the consistency of the generated answer\n",
        "        is_consistent, relevant_indices = self.verify_answer_consistency(query, answer, retrieved_indices)\n",
        "\n",
        "        if is_consistent:\n",
        "            relevant_docs = self.data.iloc[relevant_indices]\n",
        "\n",
        "            # Prepare a references section listing the relevant documents\n",
        "            references = \"\\n\".join(\n",
        "                f\"- {row['Title']} ({row['URL (Original academic paper)']})\"\n",
        "                for _, row in relevant_docs.iterrows()\n",
        "            )\n",
        "\n",
        "            # Combine the answer with references\n",
        "            answer_with_references = f\"{answer}\\n\\nReferences:\\n{references}\"\n",
        "        else:\n",
        "            # Add a warning if the answer consistency is low\n",
        "            answer_with_references = answer + \"\\n\\n[WARNING: Answer may not be consistent with source documents.]\"\n",
        "\n",
        "        return answer_with_references"
      ],
      "metadata": {
        "id": "VGyqFB5flpUd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scientific QA System"
      ],
      "metadata": {
        "id": "M10GWzVYmAfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Purpose**: Orchestrates the entire pipeline, from data loading to answer generation.\n",
        "\n",
        "**Functionality**\n",
        "- Combines the Dataset Handler, Embedding Indexer, and Query Answering components into a unified system.\n",
        "-\tManages preprocessing, embedding generation, and indexing.\n",
        "-\tResponds to user queries by leveraging the embeddings and NLP models to retrieve relevant documents and generate answers.\n",
        "- Ensures end-to-end execution with minimal manual intervention."
      ],
      "metadata": {
        "id": "2P3-c0rcQQPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main pipeline to orchestrate the scientific question-answering process\n",
        "class ScientificQASystem:\n",
        "    def __init__(self, dataset_url, nlp_model='google/flan-t5-large', qa_model='deepset/roberta-base-squad2', semantic_search_model='multi-qa-mpnet-base-cos-v1', consistency_threshold=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the entire pipeline.\n",
        "        - dataset_url: Path or URL to the dataset.\n",
        "        - nlp_model: Name of the Seq2Seq natural language model for answer generation.\n",
        "        - qa_model: Name of the Extractive QA model for extracting potential answers.\n",
        "        - semantic_search_model: Name of the sentence-transformers model used to generate embeddings.\n",
        "        - consistency_threshold: Similarity score threshold to determine consistency (default: 0.5).\n",
        "        \"\"\"\n",
        "        self.dataset_handler = DatasetHandler(dataset_url)  # Initialize the DatasetHandler\n",
        "        self.embedding_indexer = EmbeddingIndexer(semantic_search_model)  # Initialize the EmbeddingIndexer\n",
        "        self.query_answering = None  # Placeholder for the QueryAnswering system\n",
        "        self.nlp_model = nlp_model  # Model for answer generation\n",
        "        self.qa_model = qa_model  # Model for relevance filtering\n",
        "        self.semantic_search_model = semantic_search_model # Model for consistency checks\n",
        "        self.consistency_threshold = consistency_threshold # Consistency threshold\n",
        "\n",
        "    def initialize_pipeline(self):\n",
        "        \"\"\"\n",
        "        Initialize the components of the QA pipeline.\n",
        "        - Loads and preprocesses the dataset.\n",
        "        - Creates embeddings and builds a FAISS index.\n",
        "        - Prepares the query-answering system.\n",
        "        \"\"\"\n",
        "        # Load and preprocess the dataset\n",
        "        self.dataset_handler.load_dataset()\n",
        "        self.dataset_handler.check_missing_values()\n",
        "        self.dataset_handler.check_abstract_token_lengths()\n",
        "        self.dataset_handler.summarize_dataset()\n",
        "        data = self.dataset_handler.data  # Access the preprocessed dataset\n",
        "\n",
        "        # Generate embeddings for combined abstracts and build the FAISS index\n",
        "        combined_texts = data['Abstract (Original academic paper)'] + \" \" + data['Kids Abstract']\n",
        "        embeddings = self.embedding_indexer.create_embeddings(combined_texts.tolist())\n",
        "        self.embedding_indexer.build_index(embeddings)\n",
        "\n",
        "        # Initialize the query-answering system\n",
        "        self.query_answering = QueryAnswering(nlp_model=self.nlp_model, qa_model=self.qa_model, semantic_search_model=self.semantic_search_model, consistency_threshold=self.consistency_threshold, data=data)\n",
        "\n",
        "    def query(self, user_query, top_k=3):\n",
        "        \"\"\"\n",
        "        Process a user query and generate an answer.\n",
        "        - user_query: Query string from the user.\n",
        "        - top_k: Number of top results to retrieve from the index.\n",
        "        Returns: Generated answer with citations and a warning if consistency is low.\n",
        "        \"\"\"\n",
        "        # Create embeddings for the user query\n",
        "        query_embedding = self.embedding_indexer.create_embeddings([user_query])\n",
        "\n",
        "        # Search the FAISS index for the most relevant documents\n",
        "        distances, indices = self.embedding_indexer.search(query_embedding, top_k)\n",
        "\n",
        "        # Generate and return the answer\n",
        "        return self.query_answering.generate_answer(user_query, indices[0])"
      ],
      "metadata": {
        "id": "0H6zPazIlvGa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Initialization"
      ],
      "metadata": {
        "id": "bRr3PaYorTrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset URL\n",
        "dataset_url = \"hf://datasets/loukritia/science-journal-for-kids-data/science-journal-for-kids-dataset.csv\"\n",
        "\n",
        "# Initialize the ScientificQASystem\n",
        "# - This combines dataset handling, embedding, and QA tasks into a single pipeline.\n",
        "qa_system = ScientificQASystem(\n",
        "    dataset_url=dataset_url,  # URL of the dataset\n",
        "    nlp_model=\"google/flan-t5-large\",  # Seq2Seq natural language model\n",
        "    qa_model='deepset/roberta-base-squad2',  # QA model\n",
        "    semantic_search_model='multi-qa-mpnet-base-cos-v1',  # Embedding model\n",
        "    consistency_threshold=0.5 # Consistency threshold\n",
        ")\n",
        "\n",
        "# Initialize the pipeline\n",
        "# - This step performs all preparatory tasks, including loading the dataset, creating embeddings, building a FAISS index,\n",
        "#   and preparing the QA system for answering queries.\n",
        "qa_system.initialize_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrvIj0R8-nOS",
        "outputId": "d239528d-103b-4710-b97a-94e9f3455ff9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "\n",
            "Columns:\n",
            "Category, Title, Kids Abstract, Abstract (Original academic paper), URL (Original academic paper), Reading Levels\n",
            "\n",
            "Missing Values Summary:\n",
            "No missing values detected.\n",
            "\n",
            "Token Length Statistics:\n",
            "Original Abstract - Min: 35, Max: 819, Mean: 335.29\n",
            "Kids Abstract - Min: 87, Max: 435, Mean: 186.05\n",
            "Combined Abstract - Min: 207, Max: 998, Mean: 521.34\n",
            "\n",
            "Dataset Summary:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284 entries, 0 to 283\n",
            "Data columns (total 9 columns):\n",
            " #   Column                              Non-Null Count  Dtype \n",
            "---  ------                              --------------  ----- \n",
            " 0   Category                            284 non-null    object\n",
            " 1   Title                               284 non-null    object\n",
            " 2   Kids Abstract                       284 non-null    object\n",
            " 3   Abstract (Original academic paper)  284 non-null    object\n",
            " 4   URL (Original academic paper)       284 non-null    object\n",
            " 5   Reading Levels                      284 non-null    object\n",
            " 6   Original Abstract Length (tokens)   284 non-null    int64 \n",
            " 7   Kids Abstract Length (tokens)       284 non-null    int64 \n",
            " 8   Combined Abstract Length (tokens)   284 non-null    int64 \n",
            "dtypes: int64(3), object(6)\n",
            "memory usage: 20.1+ KB\n",
            "None\n",
            "\n",
            "First few rows of the dataset:\n",
            "                                            Category  \\\n",
            "0             Biodiversity-And-Conservation Articles   \n",
            "1  Biodiversity-And-Conservation Articles; Biolog...   \n",
            "2             Biodiversity-And-Conservation Articles   \n",
            "3  Biodiversity-And-Conservation Articles; Energy...   \n",
            "4  Biodiversity-And-Conservation Articles; Water-...   \n",
            "\n",
            "                                               Title  \\\n",
            "0                   Are poachers rhinos only problem   \n",
            "1             Can microbes make fruit flies stronger   \n",
            "2                 Can we save rhinos from extinction   \n",
            "3  Could this be a concrete solution to biodivers...   \n",
            "4  Counting the fish catch why dont the numbers m...   \n",
            "\n",
            "                                       Kids Abstract  \\\n",
            "0  Rhinos dont have it easy these days. Usually, ...   \n",
            "1  Have you ever thought of yourself as an ecosys...   \n",
            "2  In the past decade, poachers have increasingly...   \n",
            "3  Did you know that in some parts of the world, ...   \n",
            "4  Fish and marine animals like shrimp (well call...   \n",
            "\n",
            "                  Abstract (Original academic paper)  \\\n",
            "0  Unrelenting poaching to feed the illegal traff...   \n",
            "1  In Drosophila, diet is considered a prominent ...   \n",
            "2  The onslaught on the Worlds wildlife continues...   \n",
            "3  In coastal habitats artificial structures typi...   \n",
            "4  Here we reply to a commentary by Ye et al. (Ma...   \n",
            "\n",
            "                       URL (Original academic paper)  \\\n",
            "0  https://journals.plos.org/plosone/article?id=1...   \n",
            "1  http://journals.plos.org/plosone/article?id=10...   \n",
            "2  http://journals.plos.org/plosone/article?id=10...   \n",
            "3  http://iopscience.iop.org/article/10.1088/1748...   \n",
            "4        https://www.nature.com/articles/ncomms10244   \n",
            "\n",
            "                                      Reading Levels  \\\n",
            "0  Elementary school; Lower high school; Middle s...   \n",
            "1                   Lower high school; Middle school   \n",
            "2               Lower high school; Upper high school   \n",
            "3                   Lower high school; Middle school   \n",
            "4  Elementary school; Lower high school; Middle s...   \n",
            "\n",
            "   Original Abstract Length (tokens)  Kids Abstract Length (tokens)  \\\n",
            "0                                340                            154   \n",
            "1                                424                            250   \n",
            "2                                298                            145   \n",
            "3                                404                            176   \n",
            "4                                173                            284   \n",
            "\n",
            "   Combined Abstract Length (tokens)  \n",
            "0                                494  \n",
            "1                                674  \n",
            "2                                443  \n",
            "3                                580  \n",
            "4                                457  \n",
            "\n",
            "FAISS index built with 284 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Usage and Test"
      ],
      "metadata": {
        "id": "dZDiDJj-rY4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pipeline with a sample query\n",
        "user_query = \"Which marine mammals are one of the most extreme examples of female-biased sexual size dimorphism?\"\n",
        "\n",
        "# Perform the query\n",
        "# - This step retrieves the most relevant documents, filters them based on relevance to the query,\n",
        "#   generates an answer, and verifies its consistency with the source documents.\n",
        "answer = qa_system.query(user_query, top_k=3)\n",
        "\n",
        "# Display the query and answer\n",
        "print(\"\\nUser Query:\")\n",
        "print(user_query)\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9rWsIv-84pw",
        "outputId": "782d8b33-c69a-4b52-e9df-d34d09b08bb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query:\n",
            "Which marine mammals are one of the most extreme examples of female-biased sexual size dimorphism?\n",
            "\n",
            "Generated Answer:\n",
            "leopard seals\n",
            "\n",
            "References:\n",
            "- How can leopard seals survive climate change (https://www.frontiersin.org/articles/10.3389/fmars.2022.976019/full)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer provided by the system and the reference paper are correct.\n",
        "\n",
        "From the abstract: \"*... As females were 50% larger than their male counterparts, leopard seals are therefore one of the most extreme examples of female-biased sexual size dimorphism in marine mammals. ...*\""
      ],
      "metadata": {
        "id": "k6ul4ByCN4Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pipeline with another sample query\n",
        "user_query = \"Which stimulus may represent a cue that induces responses to nearby plants?\"\n",
        "\n",
        "# Perform the query\n",
        "# - This step retrieves the most relevant documents, filters them based on relevance to the query,\n",
        "#   generates an answer, and verifies its consistency with the source documents.\n",
        "answer = qa_system.query(user_query, top_k=3)\n",
        "\n",
        "# Display the query and answer\n",
        "print(\"\\nUser Query:\")\n",
        "print(user_query)\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwMDRPuz9IXQ",
        "outputId": "3bb943a8-2d3b-43c1-a4eb-0777c5ae6282"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query:\n",
            "Which stimulus may represent a cue that induces responses to nearby plants?\n",
            "\n",
            "Generated Answer:\n",
            "light contact with neighbouring plants\n",
            "\n",
            "References:\n",
            "- How do plants keep in touch (http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0165742)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer provided by the system and the reference paper are correct.\n",
        "\n",
        "From the abstract: \"*In natural habitats plants can be exposed to brief and light contact with neighbouring plants. This mechanical stimulus may represent a cue that induces responses to nearby plants. ...*\""
      ],
      "metadata": {
        "id": "FMzfPqraOYAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pipeline with another sample query\n",
        "user_query = \"Is there a relationship between smartphone addiction and sleep quality?\"\n",
        "\n",
        "# Perform the query\n",
        "# - This step retrieves the most relevant documents, filters them based on relevance to the query,\n",
        "#   generates an answer, and verifies its consistency with the source documents.\n",
        "answer = qa_system.query(user_query, top_k=3)\n",
        "\n",
        "# Display the query and answer\n",
        "print(\"\\nUser Query:\")\n",
        "print(user_query)\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PWuQfhvQsp7",
        "outputId": "d9216e84-7d31-4c80-f063-3a87d5cc74a8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query:\n",
            "Is there a relationship between smartphone addiction and sleep quality?\n",
            "\n",
            "Generated Answer:\n",
            "Smartphone addiction was associated with poor sleep, independent of duration of usage.\n",
            "\n",
            "References:\n",
            "- How do smartphones affect our sleep (https://doi.org/10.3389/fpsyt.2021.629407)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer provided by the system and the reference paper are correct.\n",
        "\n",
        "From the abstract: \"*Smartphone addiction was associated with poor sleep, independent of duration of usage, indicating that length of time should not be used as a proxy for harmful usage.*\""
      ],
      "metadata": {
        "id": "1G7eS1BDcfyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the pipeline with another sample query\n",
        "user_query = \"Why pharmaceuticals are increasingly found in wastewater?\"\n",
        "\n",
        "# Perform the query\n",
        "# - This step retrieves the most relevant documents, filters them based on relevance to the query,\n",
        "#   generates an answer, and verifies its consistency with the source documents.\n",
        "answer = qa_system.query(user_query, top_k=3)\n",
        "\n",
        "# Display the query and answer\n",
        "print(\"\\nUser Query:\")\n",
        "print(user_query)\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nZOXz14vnUW",
        "outputId": "27b4a24f-268a-440d-92d2-4e5d58a4377f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User Query:\n",
            "Why pharmaceuticals are increasingly found in wastewater?\n",
            "\n",
            "Generated Answer:\n",
            "Due to incomplete metabolism in humans and subsequent excretion in human waste.\n",
            "\n",
            "References:\n",
            "- Medicine in our waters so what (https://doi.org/10.1371/journal.pone.0197259)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer provided by the system and the reference paper are correct.\n",
        "\n",
        "From the abstract: \"*... Pharmaceuticals are increasingly found in wastewater and surface waters around the world, often due to incomplete metabolism in humans and subsequent excretion in human waste. ...*\""
      ],
      "metadata": {
        "id": "CxsWqD-k10Ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclutions"
      ],
      "metadata": {
        "id": "JeN2pF6ftcaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implemented solution addresses the core requirements of a scientific question-answering system capable of leveraging a document collection to provide accurate and citation-backed responses. The approach employs natural language processing (NLP) methods for retrieval, and generation. Preliminary implementation and test on simple queries demonstrates that the system can parse queries, retrieve relevant documents, and generate answers while linking them to their source. This aligns with the goal of ensuring transparency and reliability in the answers provided.\n",
        "\n",
        "Despite these achievements, the current solution has several limitations that need to be addressed for it to become production-ready.\n",
        "\n",
        "**Limitations**\n",
        "1. Lack of Experimental Evaluation:\n",
        "  -\tThe current implementation lacks a robust experimental evaluation to assess the system's performance rigorously.\n",
        "  - An error analysis mechanism to identify the root causes of system failures is absent. This would help pinpoint weaknesses in document retrieval, relevance scoring, or answer generation.\n",
        "2. Inconsistency Handling:\n",
        " - While a mechanism to detect inconsistencies was implemented, its accuracy and reliability need further validation. Incorrectly flagging valid responses or missing genuine inconsistencies could undermine trust in the system.\n",
        " - The consistency threshold should be fine-tuned.\n",
        "\n",
        "**Future Directions**\n",
        "\n",
        "1. Rigorous Evaluation and Benchmarking:\n",
        "\t-\tEstablish a comprehensive experimental setup with clear performance metrics to evaluate each stage of the pipeline. Conduct comparative experiments using domain-specific datasets.\n",
        "2. Error Analysis:\n",
        "\t-\tIntroduce systematic error categorization to identify whether errors stem from retrieval, or generation. This could guide iterative improvements in specific components of the system.\n",
        "3. System Variations:\n",
        " -\tExperiment with different model architectures for the answer generation phase or fine-tune pre-trained models for improved contextual understanding and answering.\n",
        " - Desing more advanced prompt engineering strategies, such as Few-Shot or Chain-of-Thought (CoT) prompting, to enhance the system's performance.\n",
        "\n",
        "By addressing these directions, the system can evolve from a proof-of-concept to a robust and accurate scientific question-answering tool, meeting real-world needs effectively."
      ],
      "metadata": {
        "id": "ExatfjOm5oMw"
      }
    }
  ]
}